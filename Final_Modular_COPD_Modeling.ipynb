{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec094f36",
   "metadata": {},
   "source": [
    "# Modular COPD Modeling Pipeline\n",
    "This notebook is a refactored version of the full pipeline. Each step is wrapped in modular functions, cleaned of redundancy, and ready for reuse or publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29dfe1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(path='merged_burden_risk.csv'):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna(subset=['dalys_(disability-adjusted_life_years)'])\n",
    "    df.rename(columns={'dalys_(disability-adjusted_life_years)': 'DALYs'}, inplace=True)\n",
    "    df = df.sort_values(by=['country', 'year'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a632db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_basic_features(df):\n",
    "    import numpy as np\n",
    "    df['log_gdp_per_capita'] = np.log(df['GDP PER CAPITA (USD)'] + 1)\n",
    "    df['log_population_density'] = np.log(df['Population Density'] + 1)\n",
    "    df['log_total_co2'] = np.log(df['Total CO2 Emission excluding LUCF (Mt)'] + 1)\n",
    "    df['co2_per_capita'] = df['Total CO2 Emission excluding LUCF (Mt)'] / df['Population']\n",
    "    df['no2_per_capita'] = df['Nitrogen Oxide'] / df['Population']\n",
    "    df['black_carbon_per_capita'] = df['Black Carbon'] / df['Population']\n",
    "    df['pollution_x_low_haq'] = df['co2_per_capita'] * (1 - df['HAQ_Index'].fillna(0) / 100)\n",
    "    df['year_index'] = df['year'] - df['year'].min()\n",
    "    df['lagged_dalys'] = df.groupby('country')['DALYs'].shift(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37116fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_advanced_features(df):\n",
    "    df['pm25_3yr_avg'] = df.groupby('country')['pm25_DALY'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "    df['dalys_3yr_avg'] = df.groupby('country')['DALYs'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "    df['delta_pm25'] = df.groupby('country')['pm25_DALY'].diff()\n",
    "    df['delta_black_carbon'] = df.groupby('country')['Black Carbon'].diff()\n",
    "    df['gdp_x_haq'] = df['GDP PER CAPITA (USD)'] * df['HAQ_Index'].fillna(0)\n",
    "    df['smoking_x_pm25'] = df['smoking_DALY'] * df['pm25_DALY']\n",
    "    df['haq_x_dalys_lag'] = df['HAQ_Index'].fillna(0) * df['lagged_dalys']\n",
    "    df['norm_gdp'] = df.groupby('year')['GDP PER CAPITA (USD)'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    df['norm_density'] = df.groupby('year')['Population Density'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    df['norm_haq'] = df.groupby('year')['HAQ_Index'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    df['vulnerability_index'] = (1 - df['norm_gdp']) + df['norm_density'] + (1 - df['norm_haq'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b17cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_data(df):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    feature_cols = ['log_gdp_per_capita', 'log_population_density', 'log_total_co2',\n",
    "                    'co2_per_capita', 'pollution_x_low_haq', 'year_index', 'lagged_dalys',\n",
    "                    'pm25_3yr_avg', 'delta_pm25', 'gdp_x_haq', 'smoking_x_pm25',\n",
    "                    'haq_x_dalys_lag', 'vulnerability_index']\n",
    "    df_model = df.dropna(subset=feature_cols + ['DALYs']).copy()\n",
    "    X = df_model[feature_cols]\n",
    "    y = df_model['DALYs']\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return train_test_split(X_scaled, y, test_size=0.2, random_state=42), feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f10c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    from sklearn.linear_model import Ridge, Lasso, QuantileRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    models = {\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=0.1),\n",
    "        'Quantile Regression (median)': QuantileRegressor(quantile=0.5, alpha=0.1),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'R²': r2_score(y_test, preds),\n",
    "            'MAE': mean_absolute_error(y_test, preds),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, preds))\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by='R²', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef7623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_explain(xgb_model, X_train, X_test, feature_cols):\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "    explainer = shap.Explainer(xgb_model, X_train)\n",
    "    shap_values = explainer(X_test)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    shap.summary_plot(shap_values, features=X_test, feature_names=feature_cols, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e43483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_lasso_coefficients(X_train, y_train, feature_cols):\n",
    "    from sklearn.linear_model import Ridge, Lasso\n",
    "    import pandas as pd\n",
    "    ridge = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "    lasso = Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "    ridge_coef = pd.Series(ridge.coef_, index=feature_cols)\n",
    "    lasso_coef = pd.Series(lasso.coef_, index=feature_cols)\n",
    "    return pd.DataFrame({\n",
    "        'Ridge Coefficient': ridge_coef,\n",
    "        'Lasso Coefficient': lasso_coef\n",
    "    }).sort_values(by='Ridge Coefficient', key=abs, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454ef587",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m apply_advanced_features(df)\n\u001b[1;32m      5\u001b[0m (X_train, X_test, y_train, y_test), feature_cols \u001b[38;5;241m=\u001b[39m prepare_model_data(df)\n\u001b[0;32m----> 6\u001b[0m model_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_results)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# SHAP Explainability for XGBoost\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mevaluate_models\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ridge, Lasso, QuantileRegressor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score, mean_absolute_error, mean_squared_error\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Run full pipeline\n",
    "df = load_and_clean_data()\n",
    "df = apply_basic_features(df)\n",
    "df = apply_advanced_features(df)\n",
    "(X_train, X_test, y_train, y_test), feature_cols = prepare_model_data(df)\n",
    "model_results = evaluate_models(X_train, X_test, y_train, y_test)\n",
    "print(model_results)\n",
    "\n",
    "# SHAP Explainability for XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "shap_explain(xgb_model, X_train, X_test, feature_cols)\n",
    "\n",
    "# Ridge vs Lasso\n",
    "coeffs = ridge_lasso_coefficients(X_train, y_train, feature_cols)\n",
    "coeffs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
